{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e4256b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import datetime\n",
    "from datetime import date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b422098",
   "metadata": {},
   "source": [
    "use the last 2yrs with a 1d interval to filter stocks, find stop-loss\n",
    "- try a 59day with short interval to find optimal buy/sell \n",
    "- also can try 1d interval to find optimal buy/sell, but less optimistic on this\n",
    "\n",
    "### look for high short term vol, low long term vol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e97ecf",
   "metadata": {},
   "source": [
    "Will need to be able to download stock data. \n",
    "- Limited by data availability. Ideally would be using intraday prices going back 2yrs but beyond 60-days that data isn't available from yfinance. \n",
    "Chosen to get recent and longer term price variability (PV) separately. Looking for low on both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77acd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_from_ticker(ticker):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a list of 2mo and 2yr datasets for the given ticker.\n",
    "    Both contain ~500 observations.\n",
    "    Short data comes from the last 59 days, using a 30min interval. Includes high and low price from each period. \n",
    "    Long data comes from the last 730 days, using a 1d interval. Includes high and low price from each period.\n",
    "    \"\"\"\n",
    "    \n",
    "    twoyr_start_date = datetime.datetime.now() - datetime.timedelta(730) \n",
    "    twomo_start_date = datetime.datetime.now() - datetime.timedelta(59) \n",
    "    end_date = pd.to_datetime(date.today())\n",
    "\n",
    "    short_data = yf.download(ticker, start = twomo_start_date, end = end_date, interval = \"30m\")[[\"High\", \"Low\"]]\n",
    "    long_data = yf.download(ticker, start = twoyr_start_date, end = end_date, interval = \"1d\")[[\"High\", \"Low\"]]\n",
    "    return [short_data, long_data]\n",
    "\n",
    "\n",
    "def short_long_price_variability(short_long_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calc short (2mo) and long (2yr) term price variability (price adjusted standard deviation).\n",
    "    \"\"\"\n",
    "    \n",
    "    short_data = short_long_list[0]\n",
    "    long_data = short_long_list[1]\n",
    "    \n",
    "    twomo_midpoint = short_data.assign(Mid = lambda col: (col.High + col.Low)/2) #calc period midpoint\n",
    "    twomo_mid_data = np.array(twomo_midpoint[\"Mid\"]) #create array\n",
    "    twomo_avg = twomo_mid_data.mean() #calculate average \n",
    "    twomo_std_dev = twomo_mid_data.std() #calculate deviation\n",
    "    twomo_vol = twomo_std_dev / twomo_avg #price adjusted deviation\n",
    "    \n",
    "    twoyr_midpoint = long_data.assign(Mid = lambda col: (col.High + col.Low)/2) \n",
    "    twoyr_mid_data = np.array(twoyr_midpoint[\"Mid\"])\n",
    "    twoyr_avg = twoyr_mid_data.mean()\n",
    "    twoyr_std_dev = twoyr_mid_data.std()\n",
    "    twoyr_vol = twoyr_std_dev / twoyr_avg\n",
    "    \n",
    "    return twomo_vol, twoyr_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d8276dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only check long if short matches\n",
    "def short_long_price_variability(ticker):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a list of 2mo and 2yr datasets for the given ticker.\n",
    "    Both contain ~500 observations.\n",
    "    Short data comes from the last 59 days, using a 30min interval. Includes high and low price from each period. \n",
    "    Long data comes from the last 730 days, using a 1d interval. Includes high and low price from each period.\n",
    "    \"\"\"\n",
    "    \n",
    "    twoyr_start_date = datetime.datetime.now() - datetime.timedelta(730) \n",
    "    twomo_start_date = datetime.datetime.now() - datetime.timedelta(59) \n",
    "    end_date = pd.to_datetime(date.today())\n",
    "\n",
    "    short_data = yf.download(ticker, start = twomo_start_date, end = end_date, interval = \"30m\")[[\"High\", \"Low\"]]   \n",
    "    twomo_midpoint = short_data.assign(Mid = lambda col: (col.High + col.Low)/2) #calc period midpoint\n",
    "    twomo_mid_data = np.array(twomo_midpoint[\"Mid\"]) #create array\n",
    "    twomo_avg = twomo_mid_data.mean() #calculate average \n",
    "    twomo_std_dev = twomo_mid_data.std() #calculate deviation\n",
    "    twomo_vol = twomo_std_dev / twomo_avg #price adjusted deviation\n",
    "    \n",
    "    if ((0.01 < twomo_vol) & (0.015 > twomo_vol)): #only check long data if short data fits, save runtime\n",
    "        long_data = yf.download(ticker, start = twoyr_start_date, end = end_date, interval = \"1d\")[[\"High\", \"Low\"]]\n",
    "        twoyr_midpoint = long_data.assign(Mid = lambda col: (col.High + col.Low)/2) \n",
    "        twoyr_mid_data = np.array(twoyr_midpoint[\"Mid\"])\n",
    "        twoyr_avg = twoyr_mid_data.mean()\n",
    "        twoyr_std_dev = twoyr_mid_data.std()\n",
    "        twoyr_vol = twoyr_std_dev / twoyr_avg\n",
    "    else:\n",
    "        twoyr_vol = -1 #doesn't matter really but just setting neg\n",
    "    \n",
    "    #returning a dict. originally had returning a list but caused the method to run double downstream\n",
    "    return {\"Short PV\": twomo_vol, \"Long PV\": twoyr_vol} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cda428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only check short if long matches\n",
    "def short_long_price_variability(ticker):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a list of 2mo and 2yr datasets for the given ticker.\n",
    "    Both contain ~500 observations.\n",
    "    Short data comes from the last 59 days, using a 30min interval. Includes high and low price from each period. \n",
    "    Long data comes from the last 730 days, using a 1d interval. Includes high and low price from each period.\n",
    "    \"\"\"\n",
    "    \n",
    "    twoyr_start_date = datetime.datetime.now() - datetime.timedelta(730) \n",
    "    twomo_start_date = datetime.datetime.now() - datetime.timedelta(59) \n",
    "    end_date = pd.to_datetime(date.today())\n",
    "    \n",
    "    long_data = yf.download(ticker, start = twoyr_start_date, end = end_date, interval = \"1d\")[[\"High\", \"Low\"]]\n",
    "    twoyr_midpoint = long_data.assign(Mid = lambda col: (col.High + col.Low)/2) \n",
    "    twoyr_mid_data = np.array(twoyr_midpoint[\"Mid\"])\n",
    "    twoyr_avg = twoyr_mid_data.mean()\n",
    "    twoyr_std_dev = twoyr_mid_data.std()\n",
    "    twoyr_vol = twoyr_std_dev / twoyr_avg\n",
    "    \n",
    "    if ((0.025 < twoyr_vol) & (0.035 > twoyr_vol)): #only check long data if long data fits, save runtime\n",
    "        short_data = yf.download(ticker, start = twomo_start_date, end = end_date, interval = \"30m\")[[\"High\", \"Low\"]]   \n",
    "        twomo_midpoint = short_data.assign(Mid = lambda col: (col.High + col.Low)/2) #calc period midpoint\n",
    "        twomo_mid_data = np.array(twomo_midpoint[\"Mid\"]) #create array\n",
    "        twomo_avg = twomo_mid_data.mean() #calculate average \n",
    "        twomo_std_dev = twomo_mid_data.std() #calculate deviation\n",
    "        twomo_vol = twomo_std_dev / twomo_avg #price adjusted deviation\n",
    "    else:\n",
    "        twomo_vol = -1 #doesn't matter really but just setting neg\n",
    "    \n",
    "    #returning a dict. originally had returning a list but caused the method to run double downstream\n",
    "    return {\"Short PV\": twomo_vol, \"Long PV\": twoyr_vol} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19560127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "{'Short PV': 0.013387338973652053, 'Long PV': 0.03030104479822352}\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "{'Short PV': 0.39027473494521814, 'Long PV': -1}\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "{'Short PV': 0.014442321645563024, 'Long PV': 0.04753019040683213}\n"
     ]
    }
   ],
   "source": [
    "# testing on our poster-child, expecting low price variability\n",
    "print(short_long_price_variability(\"GUD.TO\"))\n",
    "\n",
    "# expecting pretty high volatility on this one\n",
    "print(short_long_price_variability(\"BBBY\"))\n",
    "\n",
    "#works on indices as well\n",
    "print(short_long_price_variability(\"^DJI\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58b6aeb",
   "metadata": {},
   "source": [
    "It looks like BBBY has about 25x short and 20x long PV compared to GUD.TO. So the function is doing what it's supposed to.\n",
    "\n",
    "DJI has comparable short term PV, and about 1.5x long.\n",
    "\n",
    "GUD.TO is remarkably boring. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf08503",
   "metadata": {},
   "source": [
    "Retreived ticker names from https://www.nasdaq.com/market-activity/stocks/screener\n",
    "\n",
    "Filtered out stocks smaller than 300m mktcap, manually combined nasdaq and nyse tickers (total 3800)\n",
    "\n",
    "Can add Canadian stocks later if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "156a304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = pd.read_csv('/Users/kaigroden-gilchrist/Downloads/Personal_Project/SuccessiveSmallGains/Equity-ma' \\\n",
    "                      'rket-analysis/tickers.csv')\n",
    "\n",
    "tickers = tickers.rename(columns = {\"A\": \"Ticker\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4489769",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_0_1 = tickers[0:1000] #breaking it up into segments \n",
    "\n",
    "tickers_0_1['PV'] = tickers_0_1['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_0_1_df = pd.concat([tickers_0_1[['Ticker']], tickers_0_1['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_0_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63387cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_1_2 = tickers[1000:2000] \n",
    "\n",
    "tickers_1_2['PV'] = tickers_1_2['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_1_2_df = pd.concat([tickers_1_2[['Ticker']], tickers_1_2['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_1_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9888dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_2_3 = tickers[2000:3000] \n",
    "\n",
    "tickers_2_3['PV'] = tickers_2_3['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_2_3_df = pd.concat([tickers_2_3[['Ticker']], tickers_2_3['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_2_3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37827011",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_3_end = tickers[3000:] \n",
    "\n",
    "tickers_3_end['PV'] = tickers_3_end['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_3_end_df = pd.concat([tickers_3_end[['Ticker']], tickers_3_end['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_3_end_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f9d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#opting to set min variability instead of volume to filter based on liquidity but could change later \n",
    "tickers_0_1_df_filtered = tickers_0_1_df[tickers_0_1_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "tickers_1_2_df_filtered = tickers_1_2_df[tickers_1_2_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "tickers_2_3_df_filtered = tickers_2_3_df[tickers_2_3_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "tickers_3_end_df_filtered = tickers_3_end_df[tickers_3_end_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "\n",
    "print(tickers_0_1_df_filtered)\n",
    "print(tickers_1_2_df_filtered)\n",
    "print(tickers_2_3_df_filtered)\n",
    "print(tickers_3_end_df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0653ad",
   "metadata": {},
   "source": [
    "PNM is flat in the 2yr, but not enough chop for this \n",
    "TXO is a recent IPO, not enough data.\n",
    "BCSA is not flat, slowly rising. Also a recent (~1yr) IPO so not enough data\n",
    "DCRD is a SPAC (explains long term PV). Recent PV too crazy (up 47% Feb 13-20)\n",
    "HONE is great. Exactly what we're looking for (although ideally no dividend)\n",
    "LANDM looks like it was stable, but now trending downwards in the last 6mo. Not ideal. \n",
    "\n",
    "Based on HONE's Long PV of 0.34, going to expand Long PV range to 0.42 to see if I can get more hits slightly higher. Moving min to 0.3 since I've screened below and don't see anything. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "affdc58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only check short if long matches\n",
    "def short_long_price_variability(ticker):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a list of 2mo and 2yr datasets for the given ticker.\n",
    "    Both contain ~500 observations.\n",
    "    Short data comes from the last 59 days, using a 30min interval. Includes high and low price from each period. \n",
    "    Long data comes from the last 730 days, using a 1d interval. Includes high and low price from each period.\n",
    "    \"\"\"\n",
    "    \n",
    "    twoyr_start_date = datetime.datetime.now() - datetime.timedelta(730) \n",
    "    twomo_start_date = datetime.datetime.now() - datetime.timedelta(59) \n",
    "    end_date = pd.to_datetime(date.today())\n",
    "    \n",
    "    long_data = yf.download(ticker, start = twoyr_start_date, end = end_date, interval = \"1d\")[[\"High\", \"Low\"]]\n",
    "    twoyr_midpoint = long_data.assign(Mid = lambda col: (col.High + col.Low)/2) \n",
    "    twoyr_mid_data = np.array(twoyr_midpoint[\"Mid\"])\n",
    "    twoyr_avg = twoyr_mid_data.mean()\n",
    "    twoyr_std_dev = twoyr_mid_data.std()\n",
    "    twoyr_vol = twoyr_std_dev / twoyr_avg\n",
    "    \n",
    "    if ((0.03 < twoyr_vol) & (0.042 > twoyr_vol)): #only check long data if long data fits, save runtime\n",
    "        short_data = yf.download(ticker, start = twomo_start_date, end = end_date, interval = \"30m\")[[\"High\", \"Low\"]]   \n",
    "        twomo_midpoint = short_data.assign(Mid = lambda col: (col.High + col.Low)/2) #calc period midpoint\n",
    "        twomo_mid_data = np.array(twomo_midpoint[\"Mid\"]) #create array\n",
    "        twomo_avg = twomo_mid_data.mean() #calculate average \n",
    "        twomo_std_dev = twomo_mid_data.std() #calculate deviation\n",
    "        twomo_vol = twomo_std_dev / twomo_avg #price adjusted deviation\n",
    "    else:\n",
    "        twomo_vol = -1 #doesn't matter really but just setting neg\n",
    "    \n",
    "    #returning a dict. originally had returning a list but caused the method to run double downstream\n",
    "    return {\"Short PV\": twomo_vol, \"Long PV\": twoyr_vol} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49776162",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_0_1 = tickers[0:1000] #breaking it up into segments \n",
    "\n",
    "tickers_0_1['PV'] = tickers_0_1['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_0_1_df = pd.concat([tickers_0_1[['Ticker']], tickers_0_1['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_0_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e101c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_1_2 = tickers[1000:2000] \n",
    "\n",
    "tickers_1_2['PV'] = tickers_1_2['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_1_2_df = pd.concat([tickers_1_2[['Ticker']], tickers_1_2['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_1_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457daec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_2_3 = tickers[2000:3000] \n",
    "\n",
    "tickers_2_3['PV'] = tickers_2_3['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_2_3_df = pd.concat([tickers_2_3[['Ticker']], tickers_2_3['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_2_3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e9506f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_3_end = tickers[3000:] \n",
    "\n",
    "tickers_3_end['PV'] = tickers_3_end['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_3_end_df = pd.concat([tickers_3_end[['Ticker']], tickers_3_end['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_3_end_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79b337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_0_1_df_filtered = tickers_0_1_df[tickers_0_1_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "tickers_1_2_df_filtered = tickers_1_2_df[tickers_1_2_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "tickers_2_3_df_filtered = tickers_2_3_df[tickers_2_3_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "tickers_3_end_df_filtered = tickers_3_end_df[tickers_3_end_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "\n",
    "print(tickers_0_1_df_filtered)\n",
    "print(tickers_1_2_df_filtered)\n",
    "print(tickers_2_3_df_filtered)\n",
    "print(tickers_3_end_df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1a2b4e",
   "metadata": {},
   "source": [
    "EQC looked good, but just broke down after a special div payment\n",
    "JNJ is decent. Larger swings in price but may be enough of them not to rule it out.\n",
    "CBSH looks good.\n",
    "CZNC declining 1yr MA not ideal. \n",
    "ENLT recent IPO. No\n",
    "FNLC looks good.\n",
    "MLYS recent IPO\n",
    "SKWD recent IPO.\n",
    "\n",
    "CBSH and FNLC are towards the top end of the Long PV range, may as well check higher. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441ac2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#only check short if long matches\n",
    "def short_long_price_variability(ticker):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create a list of 2mo and 2yr datasets for the given ticker.\n",
    "    Both contain ~500 observations.\n",
    "    Short data comes from the last 59 days, using a 30min interval. Includes high and low price from each period. \n",
    "    Long data comes from the last 730 days, using a 1d interval. Includes high and low price from each period.\n",
    "    \"\"\"\n",
    "    \n",
    "    twoyr_start_date = datetime.datetime.now() - datetime.timedelta(730) \n",
    "    twomo_start_date = datetime.datetime.now() - datetime.timedelta(59) \n",
    "    end_date = pd.to_datetime(date.today())\n",
    "    \n",
    "    long_data = yf.download(ticker, start = twoyr_start_date, end = end_date, interval = \"1d\")[[\"High\", \"Low\"]]\n",
    "    twoyr_midpoint = long_data.assign(Mid = lambda col: (col.High + col.Low)/2) \n",
    "    twoyr_mid_data = np.array(twoyr_midpoint[\"Mid\"])\n",
    "    twoyr_avg = twoyr_mid_data.mean()\n",
    "    twoyr_std_dev = twoyr_mid_data.std()\n",
    "    twoyr_vol = twoyr_std_dev / twoyr_avg\n",
    "    \n",
    "    if ((0.042 < twoyr_vol) & (0.06 > twoyr_vol)): #only check long data if long data fits, save runtime\n",
    "        short_data = yf.download(ticker, start = twomo_start_date, end = end_date, interval = \"30m\")[[\"High\", \"Low\"]]   \n",
    "        twomo_midpoint = short_data.assign(Mid = lambda col: (col.High + col.Low)/2) #calc period midpoint\n",
    "        twomo_mid_data = np.array(twomo_midpoint[\"Mid\"]) #create array\n",
    "        twomo_avg = twomo_mid_data.mean() #calculate average \n",
    "        twomo_std_dev = twomo_mid_data.std() #calculate deviation\n",
    "        twomo_vol = twomo_std_dev / twomo_avg #price adjusted deviation\n",
    "    else:\n",
    "        twomo_vol = -1 #doesn't matter really but just setting neg\n",
    "    \n",
    "    #returning a dict. originally had returning a list but caused the method to run double downstream\n",
    "    return {\"Short PV\": twomo_vol, \"Long PV\": twoyr_vol} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad0c8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_0_1 = tickers[0:1000] #breaking it up into segments \n",
    "\n",
    "tickers_0_1['PV'] = tickers_0_1['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_0_1_df = pd.concat([tickers_0_1[['Ticker']], tickers_0_1['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_0_1_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2498a2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_1_2 = tickers[1000:2000] \n",
    "\n",
    "tickers_1_2['PV'] = tickers_1_2['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_1_2_df = pd.concat([tickers_1_2[['Ticker']], tickers_1_2['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_1_2_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2934d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_2_3 = tickers[2000:3000] \n",
    "\n",
    "tickers_2_3['PV'] = tickers_2_3['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_2_3_df = pd.concat([tickers_2_3[['Ticker']], tickers_2_3['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_2_3_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0aad5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_3_end = tickers[3000:] \n",
    "\n",
    "tickers_3_end['PV'] = tickers_3_end['Ticker'].apply(short_long_price_variability)\n",
    "\n",
    "tickers_3_end_df = pd.concat([tickers_3_end[['Ticker']], tickers_3_end['PV'].apply(pd.Series)], axis=1)\n",
    "\n",
    "print(tickers_3_end_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a2e7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers_0_1_df_filtered = tickers_0_1_df[tickers_0_1_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "tickers_1_2_df_filtered = tickers_1_2_df[tickers_1_2_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "tickers_2_3_df_filtered = tickers_2_3_df[tickers_2_3_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "tickers_3_end_df_filtered = tickers_3_end_df[tickers_3_end_df[\"Short PV\"].between(left = 0, right = 1)]\n",
    "\n",
    "print(tickers_0_1_df_filtered)\n",
    "print(tickers_1_2_df_filtered)\n",
    "print(tickers_2_3_df_filtered)\n",
    "print(tickers_3_end_df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e2f586",
   "metadata": {},
   "source": [
    "ADC looks decent. Bigger dif between highs and lows than I'd like.\n",
    "AEE looks decent. Looks like a general uptrend but not too pronounced\n",
    "AMCR looks nice. \n",
    "AVA is pretty good. Last 1yr better than previous.\n",
    "BDJ looks to be trending down slightly. \n",
    "BDX looks nice. \n",
    "BXMX looks decent. Very cyclical around the dividend. Maybe slightly trending down. \n",
    "CAG is ok. Bigger dif between highs and lows than I'd like.\n",
    "CAJ was looking good. Fell off in last 6mo. \n",
    "CAPL looks nice. Farthest end of 2yr is uptrend but since then nice. \n",
    "CHE looks good. \n",
    "CL is ok. Bigger dif between highs and lows than I'd like.\n",
    "CNA not great. Wide wavelength. \n",
    "CP looks great. \n",
    "CRBG is recent IPO. No. \n",
    "DIAX looks general downtrend. No.\n",
    "DNP no.\n",
    "DPG looks pretty good. \n",
    "DUK not great. \n",
    "EIG looks ok. Something strange big movement in Oct 2022\n",
    "ES was good. May have broken down recently. \n",
    "FCPT is ok. Bigger dif between highs and lows than I'd like\n",
    "FEI is good. Far end of 2yr rising but flat since. \n",
    "GHC is ok. Not great. \n",
    "GIB no.\n",
    "HE pretty good. \n",
    "HFRO no. \n",
    "HMN slight downtrend. Potentially ok.\n",
    "HRL no. Strong support but hitting lower highs. \n",
    "HTD not great. \n",
    "IBM is ok. Bigger dif between highs and lows than I'd like.\n",
    "IDA is ok. Bigger dif between highs and lows than I'd like.\n",
    "IGD no. \n",
    "INGR not great. \n",
    "KMB is ok. Bigger dif between highs and lows than I'd like.\n",
    "KMI is ok. Bigger dif between highs and lows than I'd like.\n",
    "LADR is ok. Don't like the big drop in Oct 22.\n",
    "MMP in a slight uptrend but still looks good. \n",
    "MSM is ok. Bigger dif between highs and lows than I'd like.\n",
    "NID no. \n",
    "NNN is ok. Bigger dif between highs and lows than I'd like.\n",
    "NVS is ok. Bigger dif between highs and lows than I'd like.\n",
    "O is ok. Don't like the big drop in Oct 22.\n",
    "OFC is ok. Don't like the big drop in Oct 22.\n",
    "PB is pretty good. \n",
    "PFS is ok. Bigger dif between highs and lows than I'd like.\n",
    "PM is ok. Bigger dif between highs and lows than I'd like.\n",
    "POR no. \n",
    "PPL looks ok, but don't like that it's hitting lower lows since Nov 21. \n",
    "PRS no. \n",
    "SCM just broke out. No. \n",
    "SON looks pretty good. \n",
    "SPH looks pretty good. \n",
    "STZ is ok. Bigger dif between highs and lows than I'd like.\n",
    "SYY is pretty good. Bigger dif between highs and lows than I'd like.\n",
    "THG no. \n",
    "VOYA just broke out. No. \n",
    "WEC is ok. Bigger dif between highs and lows than I'd like.\n",
    "WMT is ok. Bigger dif between highs and lows than I'd like.\n",
    "WPC no. \n",
    "WTRG is ok. Bigger dif between highs and lows than I'd like.\n",
    "ARCC no. \n",
    "AROW no. \n",
    "BTWN no volume. \n",
    "BWB is ok but looks too long wavelength. \n",
    "CAC no. In slight downtrend. \n",
    "CATC looks pretty good. \n",
    "CATY looks ok. \n",
    "CCNE looks pretty good. \n",
    "CGBD was flat. Breaking out now. \n",
    "CIVB no. Trending down. \n",
    "CTBI looks good. \n",
    "DCOM looks good. \n",
    "DGICA is ok. Bigger dif between highs and lows than I'd like.\n",
    "EVRG is ok. Bigger dif between highs and lows than I'd like.\n",
    "FRPH looked a little off far end of 2yr. Very good now. \n",
    "GGAA no. SPAC.\n",
    "GLPI slight uptrend. No. \n",
    "GNTY no. \n",
    "GSBC good in the last year. \n",
    "IEP good for the past year. \n",
    "INDB pretty good for the past year. \n",
    "KDP decent. \n",
    "MDLZ decent. High amplitude. \n",
    "MGEE no. \n",
    "MOFG pretty good. \n",
    "MSBI pretty good. \n",
    "NMFC no. Was good but now too big amp. \n",
    "NWBI no. Way too big amp. \n",
    "OBIO no. Spac?\n",
    "OBNK decent. Maybe slight downtrend. \n",
    "OCSL no. \n",
    "REYN big amplitude. Prob not. \n",
    "RPRX big aplitude. Prob not. \n",
    "SAFT no. Slight uptrend, large amp. \n",
    "SIRI would have been great but just crashed. \n",
    "TCPC decent. Slight downtrend. \n",
    "THFF is ok. \n",
    "TOWN is great. \n",
    "VGAS no. SAPC. \n",
    "VLGEA is decent. Don't like the big drop in Oct 22.\n",
    "WABC good in the past 1.5yr.\n",
    "WAFD is ok. \n",
    "WALD no. SPAC.\n",
    "XEL not great. High amplitude. \n",
    "\n",
    "Looks like the upper end of Long PV from this batch are generally not great (with a couple exceptions), meaning we've probably found our upper limit. \n",
    "\n",
    "The ones that look really good from this are:\n",
    "      Short     Long\n",
    "HONE  0.015847  0.034489\n",
    "CBSH  0.018631  0.041063\n",
    "FNLC  0.015954  0.040111\n",
    "AMCR  0.029136  0.043482\n",
    "BDX   0.023021  0.043537\n",
    "CAPL  0.036246  0.056862 \n",
    "CHE   0.016904  0.046703\n",
    "CP    0.019901  0.053180\n",
    "CTBI  0.038825  0.048464\n",
    "DCOM  0.030092  0.059724\n",
    "FRPH  0.023382  0.050308\n",
    "GSBC  0.016853  0.046868\n",
    "IEP   0.017944  0.054363\n",
    "INDB  0.023662  0.059701\n",
    "TOWN  0.016220  0.050351\n",
    "WABC  0.019999  0.048959\n",
    "\n",
    "Next model prob want 0.3 < Long PV < 0.55 & \n",
    "\n",
    "Can use optimization to figure out buys (at least initially?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6c187178",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_stocks = [\"GUD.TO\", \"HONE\", \"CBSH\", \"FNLC\", \"AMCR\", \"BDX\", \"CAPL\", \"CHE\", \"CP\", \"CTBI\", \"DCOM\", \"FRPH\", \"GSBC\", \"IEP\", \n",
    "               \"INDB\", \"TOWN\", \"WABC\"]\n",
    "\n",
    "start_date = datetime.date(2023,3,1) - datetime.timedelta(730) \n",
    "end_date = datetime.date(2023,3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b780d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for GUD.TO is 3.66 at buy price: 5.22 and sell price: 5.47\n",
      "That equates to a 35.05747126436782% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for HONE is 10.87 at buy price: 13.53 and sell price: 14.13\n",
      "That equates to a 40.169992609016994% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for CBSH is 46.36 at buy price: 67.99 and sell price: 71.08\n",
      "That equates to a 34.09324900720694% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for FNLC is 23.54 at buy price: 28.66 and sell price: 29.68\n",
      "That equates to a 41.06769016050244% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for AMCR is 6.2 at buy price: 11.16 and sell price: 12.71\n",
      "That equates to a 27.77777777777778% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for BDX is 120.9 at buy price: 233.17 and sell price: 250.44\n",
      "That equates to a 25.925290560535235% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for CAPL is 18.65 at buy price: 19.17 and sell price: 22.28\n",
      "That equates to a 48.643714136671875% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for CHE is 306.28 at buy price: 447.33 and sell price: 523.9\n",
      "That equates to a 34.234234234234236% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for CP is 58.58 at buy price: 70.16 and sell price: 74.67\n",
      "That equates to a 41.747434435575826% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for CTBI is 19.1 at buy price: 41.26 and sell price: 45.08\n",
      "That equates to a 23.14590402326709% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for DCOM is 23.82 at buy price: 29.49 and sell price: 35.45\n",
      "That equates to a 40.38657171922686% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for FRPH is 49.5 at buy price: 55.35 and sell price: 57.6\n",
      "That equates to a 44.71544715447154% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for GSBC is 29.72 at buy price: 57.45 and sell price: 58.94\n",
      "That equates to a 25.865970409051346% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for IEP is 25.47 at buy price: 50.0 and sell price: 54.25\n",
      "That equates to a 25.47% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for INDB is 65.41 at buy price: 77.0 and sell price: 87.9\n",
      "That equates to a 42.47402597402597% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for TOWN is 20.94 at buy price: 29.85 and sell price: 31.15\n",
      "That equates to a 35.07537688442211% annual return\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "The max profit for WABC is 27.06 at buy price: 54.64 and sell price: 61.41\n",
      "That equates to a 24.76207906295754% annual return\n"
     ]
    }
   ],
   "source": [
    "#real working one\n",
    "\n",
    "for i, stock in enumerate(best_stocks):\n",
    "    info = yf.download(stock, start=start_date, end=end_date)[[\"High\", \"Low\"]]\n",
    "    \n",
    "    info = info.round(2)\n",
    "    \n",
    "    owned = False\n",
    "    max_profit = 0.\n",
    "    best_buy = 0.\n",
    "    best_sell = 0.\n",
    "\n",
    "    min_low = np.min(info[\"Low\"])\n",
    "    max_high = np.max(info[\"High\"])\n",
    "    \n",
    "    #incrementing at 1% of buy price to reduce runtime\n",
    "    #ideally would do 0.01 for all but that's probably more trouble than it's worth at high prices\n",
    "    increment = min_low/100\n",
    "    \n",
    "    buy_prices = np.arange(min_low, max_high, increment)\n",
    "    \n",
    "    def buy(buy_price):\n",
    "        #need to add these globals since vectorizing, otherwise the program complains that it's calling  \n",
    "        #.  variables before they're defined\n",
    "        global owned\n",
    "        global max_profit\n",
    "        global best_sell\n",
    "        global best_buy\n",
    "        \n",
    "        sell_prices = np.arange(buy_price, max_high, increment)\n",
    "        \n",
    "        def sell(sell_price):\n",
    "            global owned\n",
    "            global max_profit\n",
    "            global best_sell\n",
    "            global best_buy\n",
    "            \n",
    "            profit = 0.\n",
    "            \n",
    "            for index, row in info.iterrows():\n",
    "                if (owned):\n",
    "                    if (row[\"High\"] >= sell_price):\n",
    "                        profit += (sell_price - buy_price)\n",
    "                        owned = False\n",
    "                else:\n",
    "                    if (row[\"Low\"] <= buy_price):\n",
    "                        owned = True\n",
    "                \n",
    "                if (profit > max_profit):\n",
    "                    max_profit = round(profit, 2)\n",
    "                    best_sell = round(sell_price, 2)\n",
    "                    best_buy = round(buy_price, 2)\n",
    "        \n",
    "        sell_vec = np.vectorize(sell)\n",
    "        sell_vec(sell_prices)\n",
    "                    \n",
    "    buy_vec = np.vectorize(buy)\n",
    "    buy_vec(buy_prices)\n",
    "    \n",
    "    annual_return = max_profit / (best_buy*2) * 100\n",
    "    \n",
    "    print(f\"The max profit for {stock} is {max_profit} at buy price: {best_buy} and sell price: {best_sell}\")\n",
    "    print(f\"That equates to a {annual_return}% annual return\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f4419e9",
   "metadata": {},
   "source": [
    "Next steps: decide on stop loss\n",
    "- drops X% below buy-price?\n",
    "    - different for each stock or constant X?\n",
    "        - ideally they have similar variability so can use a constant\n",
    "- drops below min_low?\n",
    "    - maybe better, that way we have defined the range in which we think we can predict prices, and if it \n",
    "      leaves that range we discard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9850d285",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff40ce83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1338f6b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaa4757",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe9bf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926c5c0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858ba0fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e3c076b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7c/glh90ckd7bdfn7cc0xbs4hgr0000gn/T/ipykernel_87761/2280545541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmax_high\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHigh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mowned\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHigh\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msell_price\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "info = yf.download(\"HONE\", start=start_date, end=end_date)[[\"High\", \"Low\"]]\n",
    "\n",
    "min_low = info.Low.min()\n",
    "max_high = info.High.max()\n",
    "    \n",
    "for date in info.keys:\n",
    "    if (owned):\n",
    "        if (info.High >= sell_price):\n",
    "            profit += sell_price - buy_price\n",
    "            owned = False\n",
    "    else:\n",
    "          if (info.Low <= buy_price):\n",
    "            owned = True\n",
    "    print(profit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70666d99",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284fb37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf #contains stock price data\n",
    "import numpy as np\n",
    "import pendulum \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "stock_info = yf.Ticker('TSLA').info\n",
    "\n",
    "market_price = stock_info['regularMarketPrice']\n",
    "previous_close_price = stock_info['regularMarketPreviousClose']\n",
    "print('market price ', market_price)\n",
    "print('previous close price ', previous_close_price)\n",
    "\n",
    "price_history = yf.Ticker('TSLA').history(period='2y', # valid periods: 1d,5d,1mo,3mo,6mo,1y,2y,5y,10y,ytd,max\n",
    "                                   interval='1mo', # valid intervals: 1m,2m,5m,15m,30m,60m,90m,1h,1d,5d,1wk,1mo,3mo\n",
    "                                   actions=False)\n",
    "time_series = list(price_history['Open'])\n",
    "dt_list = [pendulum.parse(str(dt)).float_timestamp for dt in list(price_history.index)]\n",
    "plt.style.use('dark_background')\n",
    "plt.plot(dt_list, time_series, linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a126e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def short_long_returns_volatility_ratio(short_long_list):\n",
    "    \n",
    "    \"\"\"\n",
    "    Calc the ratio between short (2mo) and long (2yr) term daily volatility of log returns.\n",
    "    Higher output means high short term vol relative to long term.\n",
    "    \n",
    "    Literature uses log returns to calculate volatility. For my purposes I'm not sure this is necessary.\n",
    "    I only care about deviation in price so a more simple approach should suffice. \n",
    "    \"\"\"\n",
    "    \n",
    "    short_data = short_long_list[0]\n",
    "    long_data = short_long_list[1]\n",
    "    \n",
    "    twomo_midpoint = short_data.assign(Mid = lambda col: (col.High + col.Low)/2) \n",
    "    twomo_log_returns = twomo_midpoint.assign(log_returns = lambda x: np.log(x.Mid / x.Mid.shift()))\n",
    "    twomo_std_dev = np.array(twomo_log_returns[\"log_returns\"])[1:].std()\n",
    "    twomo_vol = twomo_std_dev * (59**0.5) \n",
    "    \n",
    "    twoyr_midpoint = long_data.assign(Mid = lambda col: (col.High + col.Low)/2)\n",
    "    twoyr_log_returns = twoyr_midpoint.assign(log_returns = lambda x: np.log(x.Mid / x.Mid.shift()))\n",
    "    twoyr_std_dev = np.array(twoyr_log_returns[\"log_returns\"])[1:].std()\n",
    "    twoyr_vol = twoyr_std_dev * (730**0.5)\n",
    "    \n",
    "    return twomo_vol, twoyr_vol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c35cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orphan": true,
  "otter": {
   "tests": {
    "q2": {
     "name": "q2",
     "points": 2,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": 1,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": 2,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": 1,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": 1,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": 1,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
